{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2 # opencv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve\n",
    "from dataset_generator import get_waldorf_statler_mfcc_features, create_pig_image_dataset, create_swedish_chef_image_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# definitons of constants/variables\n",
    "video_file_glob_path = '../../videos/*.avi'\n",
    "audio_base_path = '../../audio/'\n",
    "pig_data_path = '../../ground_truth/pig/'\n",
    "pig_keras_path = '../../ground_truth/pig_keras/'\n",
    "pig_label_file = pig_data_path + 'labels.txt'\n",
    "swedish_chef_data_path = '../../ground_truth/swedish_chef/'\n",
    "swedish_chef_keras_path = '../../ground_truth/swedish_chef_keras/'\n",
    "swedish_chef_label_file = swedish_chef_data_path + 'labels.txt'\n",
    "evaluation_base_path = '../../evaluation/'\n",
    "\n",
    "label_map = {0: 'kermit_the_frog',\n",
    "             1: 'waldorf_and_statler',\n",
    "             2: 'pig',\n",
    "             3: 'swedish_chef',\n",
    "             4: 'none'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pig_image_dataset():\n",
    "    data = pd.DataFrame([], columns=['name', 'file_id', 'filename', 'contains_character'])\n",
    "\n",
    "    with open(pig_label_file) as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            splits = list(map(lambda line: line.strip(), line.split(\",\")))\n",
    "            name = splits[0] + '_' + splits[1]\n",
    "            labels = [int(splits[i]) for i in range(2, len(splits)) if int(splits[i]) != 4]\n",
    "        \n",
    "            filename = pig_data_path + name + '_' + splits[2] + '.jpg'\n",
    "            data = data.append({'name': name,\n",
    "                                            'file_id': int(splits[0]),\n",
    "                                            'filename': filename,\n",
    "                                            'contains_character': 1 if 2 in labels else 0}, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "def load_swedish_chef_image_dataset():\n",
    "    data = pd.DataFrame([], columns=['name', 'file_id', 'filename', 'contains_character'])\n",
    "\n",
    "    with open(swedish_chef_label_file) as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            splits = list(map(lambda line: line.strip(), line.split(\",\")))\n",
    "            name = splits[0] + '_' + splits[1]\n",
    "            labels = [int(splits[i]) for i in range(2, len(splits)) if int(splits[i]) != 4]\n",
    "        \n",
    "            filename = swedish_chef_data_path + name + '_' + splits[2] + '.jpg'\n",
    "            data = data.append({'name': name,\n",
    "                                            'file_id': int(splits[0]),\n",
    "                                            'filename': filename,\n",
    "                                            'contains_character': 1 if 3 in labels else 0}, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_test_validation_split(df):\n",
    "    video1_df = df[df['file_id'] == 1]\n",
    "    video2_df = df[df['file_id'] == 2]\n",
    "    video3_df = df[df['file_id'] == 3]\n",
    "    \n",
    "    train_v1_df, test_v1_df, val_v1_df = np.split(video1_df.sample(frac=1, random_state=42), [int(.6*len(video1_df)), int(.8*len(video1_df))])\n",
    "    train_v2_df, test_v2_df, val_v2_df = np.split(video2_df.sample(frac=1, random_state=42), [int(.6*len(video2_df)), int(.8*len(video2_df))]) \n",
    "    train_v3_df, test_v3_df, val_v3_df = np.split(video3_df.sample(frac=1, random_state=42), [int(.6*len(video3_df)), int(.8*len(video3_df))]) \n",
    "    \n",
    "    train_df = pd.concat([train_v1_df, train_v2_df, train_v3_df]).reset_index(drop=True)\n",
    "    test_df = pd.concat([test_v1_df, test_v2_df, test_v3_df]).reset_index(drop=True)\n",
    "    val_df = pd.concat([val_v1_df, val_v2_df, val_v3_df]).reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kermit image dataset already created.\n",
      "Kermit image dataset already created.\n"
     ]
    }
   ],
   "source": [
    "# create pig and swedish set image dataset if not exists (this is checked by the function itself)\n",
    "create_pig_image_dataset()\n",
    "create_swedish_chef_image_dataset()\n",
    "\n",
    "# after dataset creation, load it into dataframe\n",
    "df_pig = load_pig_image_dataset()\n",
    "df_swedish_chef = load_swedish_chef_image_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2483 validated image filenames belonging to 2 classes.\n",
      "Found 620 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create extra directories and link the image dirs because image_datasets_from_directory expects subdirectories\n",
    "Path(pig_keras_path).mkdir(exist_ok=True)\n",
    "Path(swedish_chef_keras_path).mkdir(exist_ok=True)\n",
    "if not os.path.exists(pig_keras_path + '/all'):\n",
    "    os.symlink(pig_data_path, pig_keras_path + '/all')\n",
    "    Path(pig_keras_path + '/abc').mkdir(exist_ok=True)\n",
    "if not os.path.exists(swedish_chef_keras_path + '/all'):\n",
    "    os.symlink(swedish_chef_data_path, swedish_chef_keras_path + '/all')\n",
    "    Path(swedish_chef_keras_path + '/abc').mkdir(exist_ok=True)\n",
    "\n",
    "labels_pig = df_pig.sort_values(by=['name'])['contains_character'].to_list()\n",
    "labels_swedish_chef = df_swedish_chef.sort_values(by=['name'])['contains_character'].to_list()\n",
    "\n",
    "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=25\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "df_pig['contains_character'] = df_pig['contains_character'].astype('str') # needed due to class_mode=binary\n",
    "\n",
    "train_pig = data_generator.flow_from_dataframe(\n",
    "    df_pig,\n",
    "    x_col='filename',\n",
    "    y_col='contains_character',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "test_pig = data_generator.flow_from_dataframe(\n",
    "    df_pig,\n",
    "    x_col='filename',\n",
    "    y_col='contains_character',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    subset='validation')\n",
    "\n",
    "#dataset_pig = tf.keras.preprocessing.image_dataset_from_directory(pig_keras_path,\n",
    "#                                                                  labels=labels_pig,\n",
    "#                                                                 label_mode='int')\n",
    "#dataset_swedish_chef = tf.keras.preprocessing.image_dataset_from_directory(swedish_chef_keras_path,\n",
    "#                                                                           labels=labels_swedish_chef,\n",
    "#                                                                          label_mode='int')\n",
    "                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 48,271,169\n",
      "Trainable params: 48,271,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base model without top layers\n",
    "vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "vgg16.summary()\n",
    "\n",
    "flat1 = keras.layers.Flatten()(vgg16.layers[-1].output)\n",
    "class1 = keras.layers.Dense(1024, activation='relu')(flat1)\n",
    "output = keras.layers.Dense(1, activation='softmax')(class1)\n",
    "\n",
    "model = keras.models.Model(inputs=vgg16.input, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Optional: freeze VGG16 layers\n",
    "#for layer in vgg16.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 2/39 [>.............................] - ETA: 35:05 - loss: 10.2456 - binary_accuracy: 0.3281"
     ]
    }
   ],
   "source": [
    "model.fit(train_pig, validation_data=test_pig, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
